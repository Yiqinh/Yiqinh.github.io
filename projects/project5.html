<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 5: Fun With Diffusion Models!</title>
    <link rel="stylesheet" href="../assets/css/style.css">
    
    <!-- MathJax for rendering mathematical equations -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    
    <style>
        .equation-container {
            margin: 2rem 0;
            padding: 1.5rem;
            background-color: #f5f5f5;
            border-radius: 8px;
            overflow-x: auto;
        }
        
        .equation-container .MathJax {
            font-size: 1.2em;
        }
    </style>
</head>
<body>
    <header>
        <nav>
            <h1><a href="../index.html" class="home-link">Yiqin Huang</a></h1>
            <a href="../index.html#projects">Back to Projects</a>
        </nav>
    </header>

    <main class="container project-page">
        <h2>Project 5: Fun With Diffusion Models!</h2>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/image1.png" class="pic">
                <figcaption></figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/image2.png" class="pic">
                <figcaption></figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/image3.png" class="pic">
                <figcaption></figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/image4.png" class="pic">
                <figcaption></figcaption>
            </figure>
        </div>
        
        <hr>

        <h2>Part A: The Power of Diffusion Models!</h2>

        <h3>Part 0: DeepFloyd!</h3>

        <h4>20 Inference Steps</h4>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/generated_images/steps_20/download.png" class="profile-pic">
                <figcaption>a Formula 1 race car</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/generated_images/steps_20/download (1).png" class="profile-pic">
                <figcaption>two Formula 1 race cars racing side by side</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/generated_images/steps_20/download (2).png" class="profile-pic">
                <figcaption>an airplane taking off</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/generated_images/steps_20/download (3).png" class="profile-pic">
                <figcaption>two airplanes taking off side by side</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/generated_images/steps_20/download (4).png" class="profile-pic">
                <figcaption>Apple iPhone 30 pro max</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/generated_images/steps_20/download (5).png" class="profile-pic">
                <figcaption>Mango flavored ice cream</figcaption>
            </figure>
        </div>

        <h4>100 Inference Steps</h4>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/generated_images/steps_100/download (6).png" class="profile-pic">
                <figcaption>a Formula 1 race car</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/generated_images/steps_100/download.png" class="profile-pic">
                <figcaption>two Formula 1 race cars racing side by side</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/generated_images/steps_100/download (1).png" class="profile-pic">
                <figcaption>an airplane taking off</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/generated_images/steps_100/download (2).png" class="profile-pic">
                <figcaption>two airplanes taking off side by side</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/generated_images/steps_100/download (3).png" class="profile-pic">
                <figcaption>Apple iPhone 30 pro max</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/generated_images/steps_100/download (4).png" class="profile-pic">
                <figcaption>Mango flavored ice cream</figcaption>
            </figure>
        </div>

        <p> *Random Seed: 67</p>

        <h4>Thoughts</h4>

        <p>
            The number of inference steps significantly influences the quality of the output. While a mere 20 steps is enough to get the general idea of the prompt, 
            the model is oftentimes hallucinating artifacts and mixing up different ideas in the prompt. The geometry of the image is often distorted and conflicted. 
            When the inference steps is up to 100, the quality significantly improves. The distorted look of the earlier images is almost completely gone. All the pictures look more realistic.
            However, I noticed that the generated images look much closer to training images (existing pictures) than what I am describing in the prompts. Maybe there is slight overfitting here.
        </p>

        <hr>

        <h3>Part 1: Sampling Loops</h3>

        <h4>1.1 Implementing the Forward Process</h4>

        <p> 
            In this part, we add noise to the clean image.
        </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/noise_0.png" class="profile-pic">
                <figcaption>T = 0</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/noise_250.png" class="profile-pic">
                <figcaption>T = 250</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/noise_500.png" class="profile-pic">
                <figcaption>T = 500</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/noise_750.png" class="profile-pic">
                <figcaption>T = 750</figcaption>
            </figure>
        </div>

        <h4>1.2 Classical Denoising</h4>

        <p>
            In this part, we attempt to use gaussian blurring to clean the noise we added. However, a lot of detail is lost and the image is not
        </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/noisy_250.png" class="pic">
                <figcaption></figcaption>
            </figure>
        </div>

        <h4>1.3 One-Step Denoising</h4>

        <p>In this part, we use a pretrained diffusion model to denoise our images. However, we only use one denoising step.</p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/one_step.png" class="pic">
                <figcaption>One Step Denoise Comparison</figcaption>
            </figure>
        </div>


        <h4>1.4 Iterative Denoising</h4>

        <p>
            I implemented a "strided" denoising loop to iteratively clean images. 
            This involved writing a function that iteratively applied the denoising formula to visualize the gradual improvement from noise to a clear image.
        </p>
        
        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/iter_denoise.png" class="pic">
                <figcaption>Iterative Denoise Progression Every 5th Loop</figcaption>
            </figure>
        </div>
        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/one_step_final.png" class="profile-pic">
                <figcaption>One Step Denoising</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/gauss_final.png" class="profile-pic">
                <figcaption>Gaussian Blur</figcaption>
            </figure>
        </div>

        <h4>1.5 Diffusion Model Sampling</h4>

        <p>
            In this part, we sample a pretrained diffusion model starting from pure noise and the prompt: "a high quality photo".
            We iteratively denoise to get the generated image. As you can see, the results are not perfect.
        </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/denoise_sample_naive.png" class="pic">
                <figcaption>5 Sampled Images (No CFG) </figcaption>
            </figure>
        </div>

        <h4>1.6 Classifier-Free Guidance (CFG)</h4>

        <p>
            In this part, I implemented Classifier-Free Guidance (CFG) to fix 
            the low quality of the previous images. 
            I updated the denoising function to calculate a weighted mix of conditional and unconditional noise estimates.
            I use a null prompt for the unconditioned denoising.
        </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/cfg_sample.png" class="pic">
                <figcaption>5 Sampled Images with CFG (Scale=7) </figcaption>
            </figure>
        </div>

        <h4>1.7 Image-to-image Translation</h4>

        <p>
            In this part, I edit images by adding specific amounts of noise and then running the iterative denoising process to force them back onto the natural image manifold. 
            This technique relies on the model's ability to "hallucinate" new details during reconstruction, allowing for creative edits based on how much noise was initially added.
        </p>


        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/image_translation.png" class="pic">
                <figcaption>Edits of the Campanile</figcaption>
            </figure>
        </div>

        <h4>1.7.1 Editing Hand-Drawn and Web Images</h4>

        <p>
            In this part, I try image translation with images from the web as well as hand drawn images. I use the prompt, "a high quality photo"
        </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/pepe_translation.png" class="pic">
                <figcaption>Edits of Pepe image from the Web</figcaption>
            </figure>
        </div>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/sus.png" class="pic">
                <figcaption>Edits of Hand Drawn Among Us</figcaption>
            </figure>
        </div>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/car.png" class="pic">
                <figcaption>Edits of Hand Drawn Car</figcaption>
            </figure>
        </div>

        <h4>1.7.2 Inpainting</h4>

        <p>
            In this section, I use a binary mask to generate new content in parts of the image.
        </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/campanile.png" class="pic">
                <figcaption>Original</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/inpainted_campanile.png" class="pic">
                <figcaption>Inpainted Campanile</figcaption>
            </figure>
        </div>

        <p>
            More of my own masks...
        </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/pine_mask.png" class="pic">
                <figcaption>Mask Used</figcaption>
            </figure>
        </div>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/pineapple_original.png" class="pic">
                <figcaption>Original</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/pineapple_inpainted.png" class="pic">
                <figcaption>Inpainted Pineapple</figcaption>
            </figure>
        </div>


        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/tank_mask.png" class="pic">
                <figcaption>Mask Used</figcaption>
            </figure>
        </div>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/tank.png" class="pic">
                <figcaption>Original</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/tank_impainted.png" class="pic">
                <figcaption>Inpainted Tank</figcaption>
            </figure>
        </div>

        <h4>1.7.3 Text-Conditional Image-to-image Translation</h4>

        <p>
            In this next part, I modify images by denoising the existing images using a new prompt.
        </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/missile.png" class="pic">
                <figcaption>Edit Prompt: "A missle launching"</figcaption>
            </figure>
        </div>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/colorful_bird.png" class="pic">
                <figcaption>Edit Prompt: "a colorful bird"</figcaption>
            </figure>
        </div> 
        
        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/balloon.png" class="pic">
                <figcaption>Edit Prompt: "a hot air balloon"</figcaption>
            </figure>
        </div>

        <h4>1.8 Visual Anagrams</h4>

        <p>
            I implemented Visual Anagrams to create optical illusions where an image reveals a completely different subject when flipped upside down. I modified the denoising process to compute noise estimates for two separate 
            prompts, one for the upright image and one for the flipped version, and then 
            averaged them together. This forced the diffusion model to 
            generate a single image that visually satisfies both prompts simultaneously.
        </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/gnome.png" class="pic">
                <figcaption>Prompts: "oil painting of an ice cream cone" vs "oil painting of a happy garden gnome"</figcaption>
            </figure>
        </div>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/beer.png" class="pic">
                <figcaption>Prompts: "oil painting of a pint of beer" vs "oil painting of a chandelier"</figcaption>
            </figure>
        </div>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/pineapple_anagram.png" class="pic">
                <figcaption>Prompts: "a pineapple" vs "a rocketship taking off"</figcaption>
            </figure>
        </div>

        <h4>1.9 Hybrid Images</h4>

        <p>
            In this section,
            I implemented Factorized Diffusion to create hybrid images that change appearance based on viewing distance. 
            This involved estimating noise for two separate prompts and mixing them by applying a low pass filter to one and a high pass filter to the other. 
            The resulting composite noise estimate produced an image that combines the low frequency structure of one prompt with the high frequency details of the other.
        </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/spiderman.png" class="pic">
                <figcaption>Prompts: "an oil painting of Spiderman" vs "red cherry tomatos"</figcaption>
            </figure>
        </div>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/coke.png" class="pic">
                <figcaption>Prompts: "a fire hydrant" vs "a can of coca cola"</figcaption>
            </figure>
        </div>

        <hr>

        <h3>Part B: Flow Matching from Scratch!</h3>

        <h4>1.2 Using the UNet to Train a Denoiser</h4>

        <p>
            In this part, we generate the training data pairs and add noise to clean images. We vary the amount of noise per training image by adjusting sigma.
        </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/noising_process.png" class="pic">
                <figcaption>Varying levels of noising</figcaption>
            </figure>
        </div>

        <h4>1.2.1 Training</h4>

        <p>
            In this next part, we use the noisy images to train a simple denoising network. We use a hard coded noise sigma of 0.5 to train.
        </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/simple_epoch_1.png" class="pic">
                <figcaption>Epoch 1: sample denoising results on test set</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/simple_epoch_5.png" class="pic">
                <figcaption>Epoch 5: sample denoising results on test set</figcaption>
            </figure>
        </div>


        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/simple_training.png" class="pic">
                <figcaption>Training Loss Curve</figcaption>
            </figure>
        </div>

        <h4>1.2.2 Out-of-Distribution Testing</h4>

        <p>
            We trained our denoising model on a hard coded noise sigma of 0.5. Here, we test how well our model does on out of distribution noise values. We vary the sigma value in the test dataset and see how the model performs.
        </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/ood.png" class="pic">
                <figcaption>Out of distribution noise values (test set)</figcaption>
            </figure>
        </div>

       <h4>1.2.3 Denoising Pure Noise</h4> 

       <p>
        To make our model "generative", we have it denoise pure noise images to generate new digits.
        We create training pairs between complete gaussian noise and the clean images, and try to teach the model the transform the pure noise images into a clean image from the MNIST distribution.
       </p>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/pure_noise_epoch_1.png" class="pic">
                <figcaption>Epoch 1: Denoising Pure Noise</figcaption>
            </figure>
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/pure_noise_epoch_5.png" class="pic">
                <figcaption>Epoch 5: Denoising Pure Noise</figcaption>
            </figure>
        </div>

        <div class="results-gallery">
            <figure>
                <img src="https://Yiqinh.github.io/pictures/proj_5/pure_noise_train.png" class="pic">
                <figcaption>Training Loss Curve (Pure Noise)</figcaption>
            </figure>
        </div>

        <p>
            From the images generated from pure noise, we can see that they are blurry regardless of the number of epochs, despite the training loss decreasing initially.
            The training loss gets stuck and struggles to decrease after a little bit. This is primarily because we are using all classses to train, and we are using the Mean squared error criteria.
            The MSE loss tries to minimize the distance to all the training samples, leading the model learn the average of all the classes. Thus, the images generated are very blurry and do not resemble any one particular class.
            The generated images kind of look like a mix of "6", "8", and "9". 
        </p>

        <hr>

        <h4>Part 2: Training a Flow Matching Model</h4>










    </main>

    <footer>
        <p>&copy; 2025 Yiqin Huang. All rights reserved.</p>
    </footer>
</body>
</html>