<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 4: Neural Radiance Field!</title>
    <link rel="stylesheet" href="../assets/css/style.css">
    
    <!-- MathJax for rendering mathematical equations -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    
    <style>
        .equation-container {
            margin: 2rem 0;
            padding: 1.5rem;
            background-color: #f5f5f5;
            border-radius: 8px;
            overflow-x: auto;
        }
        
        .equation-container .MathJax {
            font-size: 1.2em;
        }
    </style>
</head>
<body>
    <header>
        <nav>
            <h1><a href="../index.html" class="home-link">Yiqin Huang</a></h1>
            <a href="../index.html#projects">Back to Projects</a>
        </nav>
    </header>

    <main class="container project-page">
        <h2>Project 4: Neural Radiance Field!</h2>
        
        <hr>

        <h3>Part 0: Calibrating Your Camera and Capturing a 3D Scan</h3>

        <section>
            <h4>Part 0.1: Calibrating Your Camera</h4>
            <p> 
                I calibrated the camera by capturing 30 images of ArUco tags. 
                The script detects the 2D corner coordinates of the tags 
                in each image and maps them to their known 3D world coordinates. 
                Using these correspondences, 
                I computed the camera's intrinsic matrix and distortion coefficients with OpenCV's 
                cv2.calibrateCamera() function.            
            </p>
            
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/aruco_1.jpeg" class="profile-pic">
                    <figcaption>Tags 1</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/aruco_2.jpeg" class="profile-pic">
                    <figcaption>Tags 2</figcaption>
                </figure>
            </div>

            <hr>

            <h4>Part 0.2: Capturing a 3D Object Scan</h4>
            <p> 
                I constructed the training dataset by placing my target object next to a single printed ArUco tag, 
                which serves as a reference for calculating camera poses. Using the same calibrated camera and zoom 
                level from the previous step, I captured around 50 images. 
                I systematically moved around the object, acquiring views from various horizontal and vertical angles       
            </p>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/vb_1.jpeg" class="profile-pic">
                    <figcaption>Scan 1</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/vb_2.jpeg" class="profile-pic">
                    <figcaption>Scan 2</figcaption>
                </figure>
            </div>

            <hr>

            <h4>Part 0.3: Estimating Camera Pose</h4>
            <p> 
                To estimate the pose for each image, 
                I solved the PnP problem. 
                I detected the 2D pixel coordinates of the ArUco tag corners and mapped 
                them to their known 3D world coordinates. Using the intrinsic matrix (K) from calibration, 
                I fed these 2D/3D correspondences into cv2.solvePnP() 
                to find the rotation and translation vectors. I then inverted this 
                extrinsic matrix to get the final camera to world (c2w) 
                transformation for each image and visualized the resulting camera frustums.
            </p>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/frustum_side.png" class="profile-pic">
                    <figcaption>Side View</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/frustum_top.png" class="profile-pic">
                    <figcaption>Top View</figcaption>
                </figure>
            </div>

            <hr>

            <h4>Part 0.4: Undistorting images and creating a dataset</h4>
            <p> 
                Next, I undistorted all images using cv2.undistort() with the calibrated intrinsics from earlier.
            </p>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/undistort.png" class="pic">
                    <figcaption>Original vs Undistorted Image</figcaption>
                </figure>

            </div>

        </section>

        <hr>
        
        <section>

            <h3>Part 1: Fit a Neural Field to a 2D Image</h3>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/mlp_img.jpg" class="pic">
                    <figcaption>MLP Architecture</figcaption>
                </figure>
            </div>
            
            <p>
                For this part, I implemented a 2D neural network to fit a 
                single image, effectively mapping 2D pixel coordinates to an RGB color.
                To enable the network to learn high frequency details, 
                I first transformed the input coordinates using sinusoidal positional encoding.
                The network was trained on batches of randomly sampled pixels from the image, 
                using an Adam optimizer to minimize the Mean Squared Error.
            </p>

            <h4>Hyperparameters</h4>
            <p>
                I used a model with 4 Linear Layers, with width 256. I used an Adam optimizer with learning rate of 1e-2.
                I used a positional encoding L value of 10.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/coyote_training_prog.png" class="pic">
                    <figcaption>Test Image Training Progression</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/panda_train_prog.png" class="pic">
                    <figcaption>Panda Training Progression</figcaption>
                </figure>
            </div>

            <p>
                As can be seen in the training images, the netowrk gradually learns the details, with clear improvements by iteration 200.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/L2_64.png" class="profile-pic">
                    <figcaption>L=2, Width=64</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/L2_256.png" class="profile-pic">
                    <figcaption>L=2, Width=256</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/L10_64.png" class="profile-pic">
                    <figcaption>L=10, Width=64</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/L10_256.png" class="profile-pic">
                    <figcaption>L=10, Width=256</figcaption>
                </figure>
            </div>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/panda_L2_64.png" class="profile-pic">
                    <figcaption>L=2, Width=64</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/panda_L2_256.png" class="profile-pic">
                    <figcaption>L=2, Width=256</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/panda_L10_64.png" class="profile-pic">
                    <figcaption>L=10, Width=64</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/panda_L10_256.png" class="profile-pic">
                    <figcaption>L=10, Width=256</figcaption>
                </figure>
            </div>
            <p>
                Both positional encoding depth (L) and model width significantly impact prediction quality.
                Lower L values result in only lower frequencies being captured, resulting in a blurry mess.
                Higher L values allow linear networks to learn higher frequency details in the image.
                Lower model width result in less sharp images.
                Higher model widths result in sharper cleaner images and edges.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/panda_psnr.png" class="pic">
                    <figcaption>Panda Train PSNR (L=10, width=256, batch_size=10k) </figcaption>
                </figure>
            </div>

        </section>

        <hr>

        <section>
            <h3>Part 2: Fit a Neural Radiance Field from Multi-view Images</h3>

            <h4>Part 2.1: Create Rays from Cameras</h4>

            <p>
                For this part, I implemented the ray generation pipeline to convert 2D pixel coordinates into 
                3D rays in world space. First, I created a function to back project pixel 
                coordinates (u, v) to 3D points in the camera's coordinate system (x_c, y_c, z_c). 
                This was done by inverting the pinhole camera equation using the intrinsic matrix K. Next, 
                I implemented the camera to world (c2w) transformation, 
                which applies the extrinsic matrix [R|t] to map 3D points from the camera frame to the world frame.
                Finally, I combined these steps into a pixel_to_ray function.
            </p>

            <h4>
                Part 2.2: Sampling
            </h4>
            <p>
                 I first created a batch of rays by randomly sampling N pixel coordinates 
                 from the entire training dataset. I then used my pixel_to_ray function and the corresponding 
                 c2w matrix to generate ray_o and ray_d. Next, 
                 I discretized each ray by dividing the segment between the near and far bounds into 64 bins parameterized by t. 
                 Finally, I computed the 3D coordinates for each sampled point using p = ray_o + t * ray_d
            </p>

            <h4>
                Part 2.3: Putting the Dataloading All Together
            </h4>

            <p>
                To train the NeRF model, I implemented a dataloader and visualized the rays and points.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/random_rays.png" class="profile-pic">
                    <figcaption>100 Randomly Sampled Ray Points (Lego) </figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/one_img_rays.png" class="profile-pic">
                    <figcaption>100 Ray Points from One Image (Lego) </figcaption>
                </figure>
            </div>

            <h4>
                Part 2.4: Neural Radiance Field
            </h4>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/mlp_nerf.png" class="pic">
                    <figcaption>NeRF Network Design</figcaption>
                </figure>
            </div>

            <p>
                This network differs from the 2D version as it maps a 3D coordinate (x,y,z) 
                and a 3D view direction (d_x, d_y, d_z) to a volume density (sigma) 
                and a color (r,g,b).Both inputs were first passed through positional encoding (PE), 
                using a higher frequency (L=10) for coordinates and a lower frequency (L=4) for view directions.
            </p>

            <h4>
                Part 2.5: Volume Rendering
            </h4>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/volume.png" class="pic">
                    <figcaption></figcaption>
                </figure>
            </div>

            <p>
                Volume rendering takes the sequence of predicted colors (c_i) and densities (sigma_i) 
                from the MLP for all points along a single ray. It calculates the alpha value (alpha_i) for each point, 
                which represents the probability 
                of the ray terminating at that sample. Next, it computes transmittance (T_i), 
                which is calculated as the product of the (1 - alpha) values of all preceding samples.
                Finally, the total pixel color is computed as the weighted sum of all sample colors.
            </p>

            <hr>

            <h4>
                Rendering Results
            </h4>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/val_img[0]_iter_00000.png" class="profile-pic">
                    <figcaption>Iteration 0</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/val_img[0]_iter_00100.png" class="profile-pic">
                    <figcaption>Iteration 100</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/val_img[0]_iter_00200.png" class="profile-pic">
                    <figcaption>Iteration 200</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/val_img[0]_iter_00300.png" class="profile-pic">
                    <figcaption>Iteration 300</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/val_img[0]_iter_01999.png" class="profile-pic">
                    <figcaption>Iteration 2000</figcaption>
                </figure>  
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/psnr_training_lego.png" class="pic">
                    <figcaption></figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/psnr_validation_lego.png" class="pic">
                    <figcaption></figcaption>
                </figure>
            </div>

            <h4>The Result:</h4>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/lego_rotation_2.gif" alt="GIF">
                    <figcaption>Lego 360</figcaption>
                </figure>
            </div>

        </section>

        <hr>

        <section>
            <h3>Part 2.6: Training with your own data</h3>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_4/volleyball_2.gif" alt="GIF">
                    <figcaption>Lego 360</figcaption>
                </figure>
            </div>
        </section>


    </main>

    <footer>
        <p>&copy; 2025 Yiqin Huang. All rights reserved.</p>
    </footer>
</body>
</html>