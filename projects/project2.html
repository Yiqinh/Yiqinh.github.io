<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 2: Fun with Filters and Frequencies!</title>
    <link rel="stylesheet" href="../assets/css/style.css">
</head>
<body>
    <header>
        <nav>
            <h1><a href="../index.html" class="home-link">Yiqin Huang</a></h1>
            <a href="../index.html#projects">Back to Projects</a>
        </nav>
    </header>

    <main class="container project-page">
        <h2>Project 2: Fun with Filters and Frequencies!</h2>

        <hr>
        
        <section>
            <h3>Overview</h3>
            <p>
                This project examines key image processing methods, emphasizing 2D convolutions and frequency domain transformations. 
                It aims to develop core understandings of filter effects on images while exploring advanced techniques such as sharpening, hybrid image generation, and multi-resolution blending.
            </p>
        </section>

        <hr>
        
        <section>
            <h3>Art Gallery</h3>
        </section>
        
        <hr>

        <section>
            <h3>Part 1: Fun with Filters!</h3>
            
            <h4>Part 1.1: Convolutions from Scratch</h4>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/selfie_original.png" class="profile-pic">
                    <figcaption>Original</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/selfie_box.png" class="profile-pic">
                    <figcaption>Box Filtered</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/selfie_dx.png" class="profile-pic">
                    <figcaption>Convolve Dx</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/selfie_dy.png" class="profile-pic">
                    <figcaption>Convolve Dy</figcaption>
                </figure>
            </div>

            <h5>Implementation</h5>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/my_code.png" class="pic">
                    <figcaption>Naive Implementation of Convolve</figcaption>
                </figure>
            </div>

            <p>
                The naive implementation demonstrates convolution conceptually but is slow due to Python loops, 
                while scipy.signal.convolve2d relies on optimized C routines for efficiency, offering faster performance and robust boundary handling. My implementation is significantly slower.
                For boundaries, I pad with zeroes to ensure the same output size. Convolve2d has more boundary options like symmetric mirroring.
            </p>
            
            <h4>Part 1.2: Finite Difference Operator</h4>

            <p>
                I first convolved the cameraman image with finite difference operators to get the x and y derivatives. 
                Then, I calculated the gradient magnitude image to highlight edges. 
                Finally, I binarized this image using a threshold that balances showing real edges and suppressing noise.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/camera_dx.png" class="profile-pic">
                    <figcaption>Convolved with Dx (Vertical Edges)</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/camera_dy.png" class="profile-pic">
                    <figcaption>Convolved with Dy (Horizontal Edges)</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/camera_grad.png" class="profile-pic">
                    <figcaption>Gradient Magnitude (Noisy)</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/camera_edge.png" class="profile-pic">
                    <figcaption>Edge Image (Binary Threshold)</figcaption>
                </figure>
            </div>

            <p>
                As you can see, the edge image is slightly noisy. It picks up a lot of unnecessary details. 
                I made the threshold slightly higher (0.26) to pick up more edge details in the camera and the background.
                However, this also causes noise to be picked up on the ground.
            </p>

            <h4>Part 1.3: Derivative of Gaussian (DoG) Filter</h4>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/gauss_filter.png" class="pic">
                    <figcaption>Gaussian Filter</figcaption>
                </figure>
            </div>

            <p>
                To reduce noise from the difference operator, I first blurred the image by convolving it with a Gaussian filter. 
                Then, I repeated the edge detection steps computing derivatives and binarizing the gradient magnitude on this smoothed image for cleaner results.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/camera_original.png" class="profile-pic">
                    <figcaption>Original</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/camera_gauss.png" class="profile-pic">
                    <figcaption>Gaussian Filtered (Smooth)</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/camera_gauss_dx.png" class="profile-pic">
                    <figcaption>Convolve Smooth Image with Dx</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/camera_gauss_dy.png" class="profile-pic">
                    <figcaption>Convolve Smooth Image with Dy</figcaption>
                </figure>
            </div>

            <p>
                Then, I use these filtered image to create a cleaner edge map.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/camera_grad.png" class="profile-pic">
                    <figcaption>Gradient Magnitude (Smooth)</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/camera_gauss_edge.png" class="profile-pic">
                    <figcaption>Smooth Edge Map (Denoise)</figcaption>
                </figure>
            </div>

            <p>
                As you can see, the edge map created is a lot less noisy. The edges produced by DoG are a lot thicker and smoother compared to finite difference.
            </p>

            <p>
                We can improve efficiency and produce this same result with just ONE convolution. We can convolve the image once with the DoG filter (in both X and Y direction). The DoG filter combines the Gaussian filter and derivative filter.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/dog_filters.png" class="profile-pic">
                    <figcaption>DoG Filters</figcaption>
                </figure>
            </div>

            <p>
                As you can see below, using DoG produces the same final output as above.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/camera_grad.png" class="profile-pic">
                    <figcaption>Gradient Magnitude (DoG)</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/camera_gauss_edge.png" class="profile-pic">
                    <figcaption>Edge Map (DoG)</figcaption>
                </figure>
            </div>
        </section>

        <hr>

        <section> 
            <h3>Part 2: Fun with Frequencies!</h3>

            <h4>Part 2.1: Image "Sharpening"</h4>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/unsharp_filter.png" class="pic">
                    <figcaption>Unsharp Mask Filter</figcaption>
                </figure>
            </div>

            <p>
                The technique essentially boosts edges by deducting a smoothed image version from the original. 
                Then, it adds these high frequencies back into the original image, creating a sharper image.
                We can combine these operations by making a unsharp mask filter.
                To form the unsharp mask filter, subtract the Gaussian filter from the unit impulse filter, with σ serving as the control for sharpening intensity.
            </p>

            <p>
                unsharp_masking_filter = (1 + α) × unit_impulse_filter − α × gaussian_filter
                = (1 + α)f - α f * g = f * ((1 + α)e - αg)
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/taj_original.png" class="profile-pic">
                    <figcaption>Original Taj</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/taj_sharp.png" class="profile-pic">
                    <figcaption>Sharpened Taj</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/low_taj.png" class="profile-pic">
                    <figcaption>Blurred Taj</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/high_taj.png" class="profile-pic">
                    <figcaption>High Frequency Taj</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/plane_original.png" class="profile-pic">
                    <figcaption>Original Plane</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/plane_sharp.png" class="profile-pic">
                    <figcaption>Sharpened Plane</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/plane_low.png" class="profile-pic">
                    <figcaption>Blurred Plane</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/plane_high.png" class="profile-pic">
                    <figcaption>High Frequency Plane</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/curry_original.png" class="profile-pic">
                    <figcaption>Original Curry</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/curry_sharp.png" class="profile-pic">
                    <figcaption>Sharpened Curry</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/curry_low.png" class="profile-pic">
                    <figcaption>Blurry Curry</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/curry_high.png" class="profile-pic">
                    <figcaption>High Frequency Curry</figcaption>
                </figure>
            </div>

            <p>
                We can also compare the difference in alpha. Here, we see that increasing alpha to be too high makes an image overly sharp. This does not look good.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/plane_alpha_10.png" class="pic">
                    <figcaption>Sharpen Alpha = 10</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/plane_alpha_20.png" class="pic">
                    <figcaption>Sharpen Alpha = 20</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/plane_alpha_40.png" class="pic">
                    <figcaption>Sharpen Alpha = 40</figcaption>
                </figure>
            </div>

            <p>
                Trying to sharpen an existing blurred image also does not work. This is because the unsharp mask filter does NOT add new information. 
                Blurring the image already led to information lost, which cannot be restored. Unsharp mask filter only highlights existing information.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/curry_pre_blur.png" class="profile-pic">
                    <figcaption>Blurry Curry</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/curry_blur_sharp.png" class="profile-pic">
                    <figcaption>Sharpened Blurry Curry</figcaption>
                </figure>
            </div>

            <h4>Part 2.2: Hybrid Images</h4>

            <p>
                Hybrid images are visual illusions created by merging the low frequency information from one image with the high frequency details from another, 
                resulting in a single image that appears differently depending on the viewing distance or scale. 
                This technique leverages human visual perception, where low frequencies dominate at a distance and high frequencies become prominent up close.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/derek_cat_diagram.png" class="pic">
                    <figcaption>Hybrid Image of Derek and Nutmeg (Sigma1=8, Sigma2=12)</figcaption>
                </figure>
            </div>

            <p>
                Sigma1 (for low pass on im1): This controls the cutoff frequency for the low-frequency component. The Gaussian blur with sigma1 removes high frequencies from im1, leaving the smooth, low frequency structure. 
                A higher sigma1 lowers the cutoff, making the low pass more aggressive (more blur, so the low-frequency view dominates at even greater distances).
            </p>

            <p>
                Sigma2 (for high pass on im2): This controls the cutoff for the high frequency component. 
                A higher sigma2 lowers the cutoff, meaning you subtract more low frequencies and keep finer high frequency details.
            </p>

            <p> 
                My choices for sigma reflects the best frequency cutoffs for my monitor and the distance I can go back in my room.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/reiner.png" class="profile-pic">
                    <figcaption>Reiner (Attack on Titan)</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/titan.png" class="profile-pic">
                    <figcaption>Armored Titan (Attack on Titan)</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/hybrid_titan.png" class="profile-pic">
                    <figcaption>Reiner the Armored Titan (Attack on Titan)</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/nerd.png" class="profile-pic">
                    <figcaption>Glasses</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/clown.png" class="profile-pic">
                    <figcaption>Clown</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/hybrid_clown.png" class="profile-pic">
                    <figcaption>Glasses Clown</figcaption>
                </figure>
            </div>
            
            <h4>Part 2.3 + 2.4: Gaussian and Laplacian Stacks + Multi Resolution Blending</h4>

            <p>
                Laplacian stacks decompose an image into multiple levels representing different frequency bands, where each level captures details lost during Gaussian downsampling. 
                They enable multi resolution blending by constructing stacks for two input images and a blending mask. 
                These stacks are blended level by level to smoothly merge low and high frequency details with minimal seams. 
                The blended stack is then collapsed to reconstruct the final blended image.
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/gauss_stack_apple.png" class="pic">
                    <figcaption>Apple Gaussian Stack</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/gauss_stack_orange.png" class="pic">
                    <figcaption>Orange Gaussian Stack</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/orapple_plot.png" class="pic">
                    <figcaption>Apple Orange Blending Plot</figcaption>
                </figure>
            </div>

            <p>
                Finally, the Orapple:
            </p>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/orapple.png" class="pic">
                    <figcaption>Orapple</figcaption>
                </figure>
            </div>

            <h4>Additional Blends</h4>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/venom.jpg" class="profile-pic">
                    <figcaption>Venom</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/spiderman.jpg" class="profile-pic">
                    <figcaption>Spiderman</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/venom_blend.png" class="profile-pic">
                    <figcaption>Venom X Spiderman</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/macaron.jpeg" class="profile-pic">
                    <figcaption>Macaron</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/burger.jpeg" class="profile-pic">
                    <figcaption>Burger</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_2/macaron_blend.png" class="profile-pic">
                    <figcaption>Nice Burger</figcaption>
                </figure>
            </div>

        </section>
         
    </main>

    <footer>
        <p>&copy; 2025 Yiqin Huang. All rights reserved.</p>
    </footer>
</body>
</html>