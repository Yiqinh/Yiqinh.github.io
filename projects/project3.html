<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 3: [Auto]Stitching Photo Mosaics</title>
    <link rel="stylesheet" href="../assets/css/style.css">
    
    <!-- MathJax for rendering mathematical equations -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    
    <style>
        .equation-container {
            margin: 2rem 0;
            padding: 1.5rem;
            background-color: #f5f5f5;
            border-radius: 8px;
            overflow-x: auto;
        }
        
        .equation-container .MathJax {
            font-size: 1.2em;
        }
    </style>
</head>
<body>
    <header>
        <nav>
            <h1><a href="../index.html" class="home-link">Yiqin Huang</a></h1>
            <a href="../index.html#projects">Back to Projects</a>
        </nav>
    </header>

    <main class="container project-page">
        <h2>Project 3: [Auto]Stitching Photo Mosaics</h2>
        
        <hr>
        
        <section>
            <h3>Overview</h3>
            <p>
                This project implements image mosaicing by capturing multiple photographs and seamlessly stitching them into panoramic images. 
                The process involves computing homographies between image pairs to establish geometric correspondences, 
                then applying projective warping and resampling to align the images into a common frame. Through careful compositing, 
                the warped images are blended to create wide angle mosaics that exceed the field of view of individual photographs, 
                demonstrating the core principles behind modern panoramic photography.
            </p>
        </section>
        
        <hr>

        <section>
            <h3>A.1: Shoot the Pictures</h3>
        
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/IMG_5051.png" class="profile-pic">
                    <figcaption>Left Image</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/IMG_5052.png" class="profile-pic">
                    <figcaption>Right Image</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/street_left.png" class="profile-pic">
                    <figcaption>Left Image</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/street_right.png" class="profile-pic">
                    <figcaption>Right Image</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/building_left.png" class="profile-pic">
                    <figcaption>Left Image</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/building_right.png" class="profile-pic">
                    <figcaption>Right Image</figcaption>
                </figure>
            </div>

            <p>
                Note: Either Left or Right image is used as the fixed center of projection for the Mosaic
            </p>

        </section>

        <hr>

        <section>
            <h3>A.2: Recover Homographies</h3>
            <p>
                The homography transformation is computed by solving the following system of linear equations (for one correspondence pair in the form Ah = b):
            </p>
            
            <div class="equation-container">
                $$
                \begin{bmatrix}
                x & y & 1 & 0 & 0 & 0 & -ux & -uy \\
                0 & 0 & 0 & x & y & 1 & -vx & -vy
                \end{bmatrix}
                \times
                \begin{bmatrix}
                h_1 \\ h_2 \\ h_3 \\ h_4 \\ h_5 \\ h_6 \\ h_7 \\ h_8
                \end{bmatrix}
                =
                \begin{bmatrix}
                u \\ v
                \end{bmatrix}
                $$
            </div>

            <p>
                Where (x, y) are the coordinates in the source image and (u, v) are the corresponding coordinates in the destination image. 
                This system is solved using multiple point correspondences to determine the homography parameters h₁ through h₈. For homographies, 
                since there are 8 degrees of freedom, we need a minimum of 4 pairs of correspondences (2 equations each pair). However, it is better to
                use more than 4 correspondence points and solve a overdefined system of equations (Ah = b) for h using least squares. This accounts for slight errors in correspondence points.
            </p>

            <h4>Examples of Correspondences</h4>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/labeled_left.png" class="profile-pic">
                    <figcaption>Left Image Correspondences</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/labeled_right.png" class="profile-pic">
                    <figcaption>Right Image Correspondences</figcaption>
                </figure>
            </div>

            <p>
                Plug these correspondences into the Ah = b equation above and solving for the optimal h using least squares. Reshape h into the 3x3 homography matrix H:
            </p>

            <div class="equation-container">
                $$
                \begin{bmatrix}
                1.40257957 & -0.01136527 & -310.541182 \\
                0.26268662 & 1.25978674 & -127.907641 \\
                0.00055852 & 0.00000293 & 1.00000000
                \end{bmatrix}
                $$
            </div>

        </section>

        <section>
            <h3>A.3: Warp the Images</h3>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/warp_cookie.png" class="profile-pic">
                    <figcaption>Original Image: Cookie</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/cookie_unwarp_homography_nn.png" class="profile-pic">
                    <figcaption>Warped Nearest Neighbor</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/cookie_unwarp_homography_bl.png" class="profile-pic">
                    <figcaption>Warped Bilinear Interpolation</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/warped_noodle.png" class="profile-pic">
                    <figcaption>Original Image: Noodle</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/noodle_unwarp_homography_nn.png" class="profile-pic">
                    <figcaption>Warped Nearest Neighbor</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/noodle_unwarp_homography_bl.png" class="profile-pic">
                    <figcaption>Warped Bilinear Interpolation</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/warp_poster.png" class="profile-pic">
                    <figcaption>Original Image: Poster</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/poster_nn.png" class="profile-pic">
                    <figcaption>Warped Nearest Neighbor</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/poster_bl.png" class="profile-pic">
                    <figcaption>Warped Bilinear Interpolation</figcaption>
                </figure>
            </div>

            <h4>
                Discussion
            </h4>
            <p>
                Both nearest neighbor and bilinear interpolation methods worked well. However, bilinear interpolation
                offers subtle improvements. In the "Poster" image example, the borders of the Japanese characters
                in the bilinear interpolation method are clearer than in the nearest neighbor method. The nearest neighbor
                method makes these borders slightly more blurry and less clearly defined. When both functions are vectorized,
                the speed is identical at 0.1 seconds. Thus, the bilinear interpolation method should be used as default.
                For the naive non vectorized functions, the nearest neighbor method is more than 2 times faster.
            </p>

        </section>

        <hr>

        <section>
            <h3>A.4: Blend the Images into a Mosaic</h3>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/IMG_5051.png" class="profile-pic">
                    <figcaption>Left Image Campanile</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/IMG_5052.png" class="profile-pic">
                    <figcaption>Right Image Campanile</figcaption>
                </figure>
            </div>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/campanile.png" class="pic">
                    <figcaption>Campanile Mosaic</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/street_left.png" class="profile-pic">
                    <figcaption>Left Image Street</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/street_right.png" class="profile-pic">
                    <figcaption>Right Image Street</figcaption>
                </figure>
            </div>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/street.png" class="pic">
                    <figcaption>Street Mosaic</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/building_left.png" class="profile-pic">
                    <figcaption>Left Image Building</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/building_right.png" class="profile-pic">
                    <figcaption>Right Image Building</figcaption>
                </figure>
            </div>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/building.png" class="pic">
                    <figcaption>Building Mosaic</figcaption>
                </figure>
            </div>

            <h4> 
                Algorithm Implementation
            </h4>

            <p><strong>Step 1: Geometric Alignment</strong> - I compute a homography matrix from corresponding points between the two images, then warp the second image to align with the first image's coordinate frame.</p>
            
            <p><strong>Step 2: Canvas Setup</strong> - I calculate the bounding box needed to fit both images and determine the placement offsets for each image on this expanded canvas.</p>
            
            <p><strong>Step 3: Weight Map Generation</strong> - For the overlapping region, I use distance transforms to create blend weights. Each pixel's weight is based on its distance from the image exclusive regions. Pixels closer to image 1's exclusive area favor image 1, and vice versa.</p>
            
            <p><strong>Step 4: Multi Resolution Decomposition</strong> - I build 2 level Laplacian pyramids for both images</p>
            
            <p><strong>Step 5: Pyramid Blending</strong> - At each pyramid level, I blend the corresponding frequency bands using the weight maps, ensuring smooth transitions.</p>
            
            <p><strong>Step 6: Reconstruction</strong> - I reconstruct the final panorama from the blended pyramid levels.</p>
            
            <p>This blending approach eliminates visible seams by blending different frequency components appropriately.</p>

        </section>

        <hr>

        <section>
            <h3>B.1: Harris Corner Detection</h3>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/harris_mindist_1.png" class="profile-pic">
                    <figcaption>Harris Corner (min_dist=1)</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/harris_mindist_10.png" class="profile-pic">
                    <figcaption>Harris Corner (min_dist=10)</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/anms_mindist_1.png" class="profile-pic">
                    <figcaption>ANMS Filtered (min_dist=1)</figcaption>
                </figure>
            </div>

            <p>
                The Harris detector evaluates the local gradient structure to find points where intensity changes occur 
                in multiple directions. However, these corners tend to cluster heavily in high texture regions, even with a higher minimum distance.
            </p>

            <p>
                I applied Adaptive Non Maximal Suppression (ANMS) to select well distributed corners across the image. 
                ANMS ranks corners by their suppression radius, which is the distance to the nearest stronger corner (scaled by c_robust factor). 
                This ensures features are spread throughout the scene rather than concentrated in textured areas.
            </p>

            <p>
                The computational overhead of ANMS was noticeable but worthwhile given the 
                substantial improvement in feature distribution quality.
            </p>

        </section>

        <section>
            <h3>B.2: Feature Descriptor Extraction</h3>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/campanile_descriptors.png" class="profile-pic">
                    <figcaption>Labeled Correspondences (Descriptors)</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/descriptors_list.png" class="pic">
                    <figcaption>Feature Descriptor Patches</figcaption>
                </figure>
            </div>

            <p>
                I extracted 8×8 feature descriptors for each auto detected corner by downsampling from a 40×40 pixel window. 
                Each descriptor was bias/gain normalized (zero mean, unit standard deviation) to achieve invariance to intensity changes. 
                Above images show example extracted descriptors from various image regions.
            </p>

        </section>

        <section>
            <h3>B.3: Feature Matching</h3>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/feature_match_A.png" class="profile-pic">
                    <figcaption></figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/feature_match_B.png" class="profile-pic">
                    <figcaption></figcaption>
                </figure>
            </div>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/street_A.png" class="profile-pic">
                    <figcaption></figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/street_B.png" class="profile-pic">
                    <figcaption></figcaption>
                </figure>
            </div>
            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/building_A.png" class="profile-pic">
                    <figcaption></figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/building_B.png" class="profile-pic">
                    <figcaption></figcaption>
                </figure>
            </div>
            
            <p>
                To establish correspondences between images, I implemented feature matching using nearest neighbor distance ratios. 
                For each descriptor in the first image, I computed its Euclidean distance to all descriptors in the second image. 
                Following Lowe's ratio test, a match was accepted only if 
                the distance to the nearest neighbor was sufficiently smaller than the distance to the second nearest neighbor, using a hard coded threshold.
                The above images show the automatically matched correspondences.
            </p>

        </section>

        <section>
            <h3>B.4: RANSAC for Robust Homography</h3>

            <p>
                While feature matching produces many correspondences, not all pairs are correctly matched.
                
                Outliers from mismatched features can severely degrade homography estimation if all matches are used naively. 
                To compute a robust homography, I use RANSAC.
                This algorithm iteratively fits homographies to random subsets of matches and identifies the largest set of 
                inliers consistent with a single transformation. The homography is then recomputed using only 
                these inliers.
            </p>
    
            <div>
                <h4>RANSAC Algorithm</h4>
            
                <ol>
                    <li><strong>Randomly sample 4 point correspondences</strong> from the matched feature pairs. Four points are the minimum needed to uniquely determine a homography matrix.</li>
                    
                    <li><strong>Compute the homography H</strong> using these 4 point pairs. This can be done by solving a system of linear equations.</li>
                    
                    <li><strong>Transform all points</strong> from image 1 using the computed homography H and calculate their distance to the corresponding points in image 2.</li>
                    
                    <li><strong>Count inliers</strong>: points are inliers if their reprojection error (distance between transformed point and actual correspondence) is below the threshold.</li>
                                
                    <li><strong>Repeat previous steps</strong> for the specified number of iterations for the largest set of inlier points</li>
                    
                    <li><strong>Recompute the final homography</strong> using all inliers from the best model (least squares)</li>
                </ol>

            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/auto_campanile.png" class="profile-pic">
                    <figcaption>Auto Stitched Campanile Mosaic</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/campanile.png" class="profile-pic">
                    <figcaption>Manual Stitched Campanile Mosaic</figcaption>
                </figure>
            </div>

            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/auto_street.png" class="profile-pic">
                    <figcaption>Auto Stitched Street Mosaic</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/street.png" class="profile-pic">
                    <figcaption>Manual Stitched Street Mosaic</figcaption>
                </figure>
            </div>


            <div class="results-gallery">
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/part_b/auto_building.png" class="profile-pic">
                    <figcaption>Auto Stitched Building Mosaic</figcaption>
                </figure>
                <figure>
                    <img src="https://Yiqinh.github.io/pictures/proj_3/building.png" class="profile-pic">
                    <figcaption>Manual Stitched Building Mosaic</figcaption>
                </figure>
            </div>

            <p>
                The automatically stitched mosaics achieve quality comparable to
                manually stitched results, if not better. This is particularly evident in the "Building" mosaic, where the 
                overlapping region contains many trees and plants. While such textured vegetation lacks distinct landmarks 
                for manual selection, the automatic corner detector reliably identifies subtle gradient features, 
                demonstrating the algorithm's ability to handle regions that are more challenging for human annotation. The automatic
                mosaic is less blurry on the border.
            </p>

        </section>


    </main>

    <footer>
        <p>&copy; 2025 Yiqin Huang. All rights reserved.</p>
    </footer>
</body>
</html>